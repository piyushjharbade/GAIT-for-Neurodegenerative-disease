{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb16394-c69d-42f0-8c47-48fcb0a7d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f2b4c-c693-4491-acb6-d6cd570e7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define BConvLSTM and Hidden Attention Module\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ConvLSTM Cell (single direction)\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3, padding=1):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        # Convolution for input-to-state and state-to-state transitions\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,\n",
    "            out_channels=4 * hidden_dim,  # For input, forget, cell, output gates\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # [batch, input_dim + hidden_dim, height, width]\n",
    "        combined_conv = self.conv(combined)\n",
    "        # Split into gates\n",
    "        cc_i, cc_f, cc_c, cc_o = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        # Gate activations\n",
    "        i = torch.sigmoid(cc_i)  # Input gate\n",
    "        f = torch.sigmoid(cc_f)  # Forget gate\n",
    "        o = torch.sigmoid(cc_o)  # Output gate\n",
    "        c_next = f * c_cur + i * torch.tanh(cc_c)  # Cell state\n",
    "        h_next = o * torch.tanh(c_next)  # Hidden state\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "# Bidirectional ConvLSTM\n",
    "class BConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3, padding=1):\n",
    "        super(BConvLSTM, self).__init__()\n",
    "        self.forward_cell = ConvLSTMCell(input_dim, hidden_dim, kernel_size, padding)\n",
    "        self.backward_cell = ConvLSTMCell(input_dim, hidden_dim, kernel_size, padding)\n",
    "        # Final convolution to combine forward and backward outputs\n",
    "        self.conv_out = nn.Conv2d(hidden_dim * 2, hidden_dim, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, channels, height, width]\n",
    "        batch_size, _, height, width = x.size()\n",
    "        # Initialize hidden states\n",
    "        h_f, c_f = self.forward_cell.init_hidden(batch_size, (height, width))\n",
    "        h_b, c_b = self.backward_cell.init_hidden(batch_size, (height, width))\n",
    "        \n",
    "        # Forward pass\n",
    "        h_forward = []\n",
    "        for t in range(1):  # Single time step (as input is a single feature map)\n",
    "            h_f, c_f = self.forward_cell(x, cur_state=[h_f, c_f])\n",
    "            h_forward.append(h_f)\n",
    "        h_forward = h_forward[0]  # [batch, hidden_dim, height, width]\n",
    "        \n",
    "        # Backward pass\n",
    "        h_backward = []\n",
    "        for t in range(1):  # Single time step\n",
    "            h_b, c_b = self.backward_cell(x, cur_state=[h_b, c_b])\n",
    "            h_backward.append(h_b)\n",
    "        h_backward = h_backward[0]  # [batch, hidden_dim, height, width]\n",
    "        \n",
    "        # Combine forward and backward\n",
    "        h_combined = torch.cat([h_forward, h_backward], dim=1)  # [batch, hidden_dim*2, height, width]\n",
    "        output = self.conv_out(h_combined)  # [batch, hidden_dim, height, width]\n",
    "        return output\n",
    "\n",
    "# Hidden Attention Module with BConvLSTM\n",
    "class HiddenAttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(HiddenAttentionModule, self).__init__()\n",
    "        # First Conv Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Second Conv Layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        # SE Block\n",
    "        self.se_block = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(out_channels, out_channels // 16, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels // 16, out_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # Shortcut Connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        # BConvLSTM\n",
    "        self.bconvlstm = BConvLSTM(input_dim=out_channels, hidden_dim=out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        # Conv Path\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        # SE Block\n",
    "        se = self.se_block(out)\n",
    "        out = out * se\n",
    "        # Shortcut\n",
    "        identity = self.shortcut(identity)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        # BConvLSTM\n",
    "        out = self.bconvlstm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d171b0d9-683f-4265-8473-ebc1a682abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define HA-ResNet Model (with Dropout)\n",
    "class HA_ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(HA_ResNet, self).__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.ham1 = HiddenAttentionModule(64, 64, stride=1)\n",
    "        self.ham2 = HiddenAttentionModule(64, 128, stride=2)\n",
    "        self.ham3 = HiddenAttentionModule(128, 256, stride=2)\n",
    "        self.ham4 = HiddenAttentionModule(256, 512, stride=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.ham1(x)\n",
    "        x = self.ham2(x)\n",
    "        x = self.ham3(x)\n",
    "        x = self.ham4(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82d735-d905-42cf-99b1-68a6791a91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define Custom Dataset\n",
    "class GaitGAFDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75379ae4-e1fd-4d48-9d49-c7799dbeb871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyus\\mlenv\\gaitenv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb68380-4c1f-415e-bc4b-d29cdeb285ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyus\\mlenv\\gaitenv\\gait-in-neurodegenerative-disease-database-1.0.0\\gaf_images_augmented\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"gait-in-neurodegenerative-disease-database-1.0.0/gaf_images_augmented/\")  # Adjust path\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb878ede-7327-4a1b-af6b-58ccc7036c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['als', 'control', 'hunt', 'park']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fbded7f-9a7c-430f-bb31-cc2ff976195b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of als: ['DoubleSupport', 'DoubleSupport%', 'L_Stance', 'L_Stance%', 'L_Stride', 'L_Swing', 'L_Swing%', 'R_Stance', 'R_Stance%', 'R_Stride', 'R_Swing', 'R_Swing%']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cls_dir = \"als\"  # Try \"ALS\" if lowercase fails\n",
    "if os.path.exists(cls_dir):\n",
    "    print(f\"Contents of {cls_dir}:\", os.listdir(cls_dir))\n",
    "else:\n",
    "    print(f\"{cls_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf89dc12-262c-48ec-8853-3ca1363d2812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in als\\L_Stride: 52\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "feature_dir = os.path.join(\"als\", \"L_Stride\")  # Try \"ALS\" if needed\n",
    "if os.path.exists(feature_dir):\n",
    "    images = [f for f in os.listdir(feature_dir) if f.endswith('.png')]\n",
    "    print(f\"Images in {feature_dir}:\", len(images))\n",
    "else:\n",
    "    print(f\"{feature_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d805a0ad-f54f-4f70-a034-041ec23b7b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset mapping for HA-ResNet training...\n",
      "Collected 52 images for als/L_Stride\n",
      "Collected 52 images for als/R_Stride\n",
      "Collected 52 images for als/L_Swing\n",
      "Collected 52 images for als/R_Swing\n",
      "Collected 52 images for als/L_Swing%\n",
      "Collected 52 images for als/R_Swing%\n",
      "Collected 52 images for als/L_Stance\n",
      "Collected 52 images for als/R_Stance\n",
      "Collected 52 images for als/L_Stance%\n",
      "Collected 52 images for als/R_Stance%\n",
      "Collected 52 images for als/DoubleSupport\n",
      "Collected 52 images for als/DoubleSupport%\n",
      "Collected 64 images for control/L_Stride\n",
      "Collected 64 images for control/R_Stride\n",
      "Collected 64 images for control/L_Swing\n",
      "Collected 64 images for control/R_Swing\n",
      "Collected 64 images for control/L_Swing%\n",
      "Collected 64 images for control/R_Swing%\n",
      "Collected 64 images for control/L_Stance\n",
      "Collected 64 images for control/R_Stance\n",
      "Collected 64 images for control/L_Stance%\n",
      "Collected 64 images for control/R_Stance%\n",
      "Collected 64 images for control/DoubleSupport\n",
      "Collected 64 images for control/DoubleSupport%\n",
      "Collected 80 images for hunt/L_Stride\n",
      "Collected 80 images for hunt/R_Stride\n",
      "Collected 80 images for hunt/L_Swing\n",
      "Collected 80 images for hunt/R_Swing\n",
      "Collected 80 images for hunt/L_Swing%\n",
      "Collected 80 images for hunt/R_Swing%\n",
      "Collected 80 images for hunt/L_Stance\n",
      "Collected 80 images for hunt/R_Stance\n",
      "Collected 80 images for hunt/L_Stance%\n",
      "Collected 80 images for hunt/R_Stance%\n",
      "Collected 80 images for hunt/DoubleSupport\n",
      "Collected 80 images for hunt/DoubleSupport%\n",
      "Collected 60 images for park/L_Stride\n",
      "Collected 60 images for park/R_Stride\n",
      "Collected 60 images for park/L_Swing\n",
      "Collected 60 images for park/R_Swing\n",
      "Collected 60 images for park/L_Swing%\n",
      "Collected 60 images for park/R_Swing%\n",
      "Collected 60 images for park/L_Stance\n",
      "Collected 60 images for park/R_Stance\n",
      "Collected 60 images for park/L_Stance%\n",
      "Collected 60 images for park/R_Stance%\n",
      "Collected 60 images for park/DoubleSupport\n",
      "Collected 60 images for park/DoubleSupport%\n",
      "\n",
      "Total images in dataset: 3072 (expected 3072)\n",
      "Example entry: ('C:\\\\Users\\\\piyus\\\\mlenv\\\\gaitenv\\\\gait-in-neurodegenerative-disease-database-1.0.0\\\\gaf_images_augmented\\\\als\\\\L_Stride\\\\als1.png', 0)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Dataset Mapping for HA-ResNet Training\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    \"als\": 0,\n",
    "    \"control\": 1,\n",
    "    \"hunt\": 2,\n",
    "    \"park\": 3\n",
    "}\n",
    "\n",
    "# Base directory\n",
    "base_dir = os.path.join(os.getcwd())\n",
    "\n",
    "# Collect image paths and labels\n",
    "dataset_mapping = []\n",
    "groups = [\"als\", \"control\", \"hunt\", \"park\"]\n",
    "feature_columns = [\n",
    "    \"L_Stride\", \"R_Stride\", \"L_Swing\", \"R_Swing\", \"L_Swing%\", \"R_Swing%\", \n",
    "    \"L_Stance\", \"R_Stance\", \"L_Stance%\", \"R_Stance%\", \"DoubleSupport\", \"DoubleSupport%\"\n",
    "]\n",
    "\n",
    "print(\"Creating dataset mapping for HA-ResNet training...\")\n",
    "for group in groups:\n",
    "    label = label_mapping[group]\n",
    "    for feature in feature_columns:\n",
    "        feature_dir = os.path.join(base_dir, group, feature)\n",
    "        if not os.path.exists(feature_dir):\n",
    "            print(f\"Warning: Directory not found: {feature_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Find all .png files in the feature directory\n",
    "        image_paths = glob.glob(os.path.join(feature_dir, \"*.png\"))\n",
    "        for image_path in image_paths:\n",
    "            dataset_mapping.append((image_path, label))\n",
    "        \n",
    "        print(f\"Collected {len(image_paths)} images for {group}/{feature}\")\n",
    "\n",
    "# Verify total images\n",
    "print(f\"\\nTotal images in dataset: {len(dataset_mapping)} (expected 3072)\")\n",
    "if len(dataset_mapping) != 3072:\n",
    "    print(f\"Warning: Expected 3072 images (64 subjects × 12 features × 4 versions), but found {len(dataset_mapping)}.\")\n",
    "\n",
    "# Example entry\n",
    "if dataset_mapping:\n",
    "    print(f\"Example entry: {dataset_mapping[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db68a8e-835b-4aeb-abcc-8e049d8a9da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 2149\n",
      "Validation dataset size: 308\n",
      "Test dataset size: 615\n",
      "Training class distribution: Counter({2: 672, 1: 537, 3: 504, 0: 436})\n",
      "Validation class distribution: Counter({2: 96, 1: 77, 3: 72, 0: 63})\n",
      "Test class distribution: Counter({2: 192, 1: 154, 3: 144, 0: 125})\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Prepare Dataset and DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Extract image paths and labels from dataset_mapping (from Cell 5)\n",
    "image_paths = [item[0] for item in dataset_mapping]\n",
    "labels = [item[1] for item in dataset_mapping]\n",
    "\n",
    "# Split dataset into train, validation, and test sets (70% train, 10% val, 20% test)\n",
    "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    train_val_paths, train_val_labels, test_size=0.1/0.8, stratify=train_val_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = GaitGAFDataset(train_paths, train_labels, transform=transform)\n",
    "val_dataset = GaitGAFDataset(val_paths, val_labels, transform=transform)\n",
    "test_dataset = GaitGAFDataset(test_paths, test_labels, transform=transform)\n",
    "\n",
    "# Compute class weights for WeightedRandomSampler (to handle class imbalance)\n",
    "label_counts = Counter(train_labels)\n",
    "num_samples = len(train_labels)\n",
    "class_weights = {i: num_samples / (len(label_counts) * count) for i, count in label_counts.items()}\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders with optimized settings\n",
    "batch_size = 32  # Increased from 32 to better utilize GPU\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "# Print dataset sizes and class distributions\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Training class distribution: {Counter(train_labels)}\")\n",
    "print(f\"Validation class distribution: {Counter(val_labels)}\")\n",
    "print(f\"Test class distribution: {Counter(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "056cba5f-65f0-4f00-b625-907f900b07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training and Evaluation Functions (Updated for Overfitting Check and Efficiency)\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    best_val_f1 = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # Keep predictions and labels on GPU for now\n",
    "            train_preds.extend(preds.tolist())\n",
    "            train_true.extend(labels.tolist())\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = accuracy_score(train_true, train_preds)\n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(\n",
    "            train_true, train_preds, average='weighted', zero_division=0\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_preds.extend(preds.tolist())\n",
    "                val_true.extend(labels.tolist())\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "            val_true, val_preds, average='weighted', zero_division=0\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train - Loss: {epoch_loss:.4f}, Accuracy: {train_accuracy:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}\")\n",
    "        print(f\"Unique predicted classes (val): {np.unique(val_preds)}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), 'best_ha_resnet.pth')\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            preds.extend(predicted.tolist())\n",
    "            true.extend(labels.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(true, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1-Score: {f1:.4f}\")\n",
    "    print(f\"Unique predicted classes (test): {np.unique(preds)}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['ALS', 'Control', 'Huntington', 'Parkinson'],\n",
    "                yticklabels=['ALS', 'Control', 'Huntington', 'Parkinson'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344250c-adc0-4490-a2f0-efeb6f5b3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main Execution (Train and Evaluate Model with Profiling)\n",
    "import torch.profiler\n",
    "\n",
    "# Initialize model\n",
    "model = HA_ResNet(num_classes=4).to(device)\n",
    "\n",
    "# Class weights for imbalanced dataset (inverse of class frequencies)\n",
    "class_weights = torch.tensor([3072/624, 3072/768, 3072/960, 3072/720], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.0001  # As per paper\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5  # Reduced for profiling; increase back to 20 after optimization\n",
    "\n",
    "# Set up profiler\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=2),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/ha_resnet'),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load('best_ha_resnet.pth'))\n",
    "evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Print profiling results\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
